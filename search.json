[{"path":"https://jessicabolin.github.io/sdmclimateforecasts/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2026 sdmclimateforecasts authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/detailed-diags.html","id":"calculate-some-detailed-sdm-fit-diagnostics","dir":"Articles","previous_headings":"","what":"Calculate some detailed SDM fit diagnostics","title":"3. Detailed diagnostics","text":"Load data split test training sets different levels cross-validation #!!RW: need define functions, save source 2_Fit_model.R","code":"# Load biological observation data with environmental covariates extracted # This is now updated to include variables extracted at a lag of 1 year, which are used to calculate # persistence skill (variables labeled with \"_lag1\") # Is assumed that the columns \"lon\", \"lat\", and \"date\" are present and complete obs <- readRDS(\"./data/combinedBioObs_envExtracted.rds\") # If observations don't have a \"year\", \"month\" and \"quarter\" columns, add them. Will need a \"date\" column to get them if(!\"year\" %in% colnames(obs)) {   obs$year <- year(obs$date)   } if(!\"month\" %in% colnames(obs)) {   obs$month <- month(obs$date)   } if(!\"quarter\" %in% colnames(obs)) {   obs$quarter <- quarter(obs$date)   }  # I'm missing predictor variables for 2024, so I'm trimming that year off obs <- subset(obs, year <= 2023) # Define the target variable: here presence/absence of anchovy obs$pa <- obs$anchPA  # Define the training and test forecast years yrs <- unique(sort(subObs$year)) # Years in the observational dataset terminalYr <- max(yrs) - yrsToForecast # The last year of training data  train <- subset(subObs, year <= terminalYr) test <- subset(subObs, year > terminalYr & year <= (terminalYr + yrsToForecast)) # Observations after the training data ends  # Assume your dataset is named 'data' and the response variable is 'presence' # Create an 75-25 train-test split trainIndex <- createDataPartition(DataInput_Fit$PresAbs, p = 0.75, list = FALSE) # Subset data train <- DataInput_Fit[trainIndex, ] test <- DataInput_Fit[-trainIndex, ] # Leave-one-out cross-validation SDR.loo.eval <-LOO_eval(DataInput_Fit,gbm.x=gbm.x, gbm.y=\"PresAbs\",lr=0.01, tc=3)  # test with 25% of the full dataset SDR.7525.eval <- eval_7525(DataInput_Fit,gbm.x=gbm.x, gbm.y=\"PresAbs\",lr=0.01, tc=3)  # comare to full model fit  #!!RW: Is this just the same as buildSDM()? SDR.100.eval <- eval_100_percent(DataInput_Fit,gbm.x=gbm.x, gbm.y=\"PresAbs\",lr=0.01, tc=3)"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/fit-model.html","id":"fit-an-sdm-model-and-calculate-basic-diagnostics","dir":"Articles","previous_headings":"","what":"Fit an SDM model and calculate basic diagnostics","title":"2. Simple model fitting","text":"Load training dataset fit ","code":"# Load biological observation data with environmental covariates extracted # This is now updated to include variables extracted at a lag of 1 year, which are used to calculate # persistence skill (variables labeled with \"_lag1\") # Is assumed that the columns \"lon\", \"lat\", and \"date\" are present and complete obs <- readRDS(\"./data/combinedBioObs_envExtracted.rds\") # If observations don't have a \"year\", \"month\" and \"quarter\" columns, add them. Will need a \"date\" column to get them if(!\"year\" %in% colnames(obs)) {   obs$year <- year(obs$date)   } if(!\"month\" %in% colnames(obs)) {   obs$month <- month(obs$date)   } if(!\"quarter\" %in% colnames(obs)) {   obs$quarter <- quarter(obs$date)   }  # I'm missing predictor variables for 2024, so I'm trimming that year off obs <- subset(obs, year <= 2023) # Define the target variable: here presence/absence of anchovy obs$pa <- obs$anchPA  # Define the training and test forecast years yrs <- unique(sort(subObs$year)) # Years in the observational dataset terminalYr <- max(yrs) - yrsToForecast # The last year of training data  train <- subset(subObs, year <= terminalYr) test <- subset(subObs, year > terminalYr & year <= (terminalYr + yrsToForecast)) # Observations after the training data ends  mod1 <- buildSDM(sdmType = sdmType, train = train, varNames = varNames, targetName = targetName,                     k = k, tc = tc, lr = lr, max.trees = max.trees) summary(mod1) # If you want to check convergence etc. But GAMs/BRTs nearly always converge unless parameters v inappropriate   # gbm.step prints model convergence progress as it goes, so you'll see if the number of trees is too small (< ~ 1500)    # For GAMS concurvity(mod1) gam.check(mod1)  #Percent deviance explained: SINGLE model #((null deviance - residual deviance)/null deviance)*100 dev_eval=function(model_object){   null <- model_object$self.statistics$mean.null   res <- model_object$self.statistics$mean.resid   dev=((null - res)/null)*100    return(dev) } dev_eval(mod1)# deviance explained by the model"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/fit-model.html","id":"evaluate-diagnostics-of-models-fit-to-data","dir":"Articles","previous_headings":"Fit an SDM model and calculate basic diagnostics","what":"Evaluate diagnostics of model’s fit to data","title":"2. Simple model fitting","text":"Calculate basic diagnostics Visualize results","code":"# Return some simple diagnostics evalOutputs <- runDiagnostics(mod = mod1, max.trees = max.trees) # For GAMs plot(mod1, pages = 1)  # For BRTs gbm.plot(res1.tc3.lr01, smooth=TRUE, plot.layout = c(4,4), write.title=T) #response curves/ fitted functions  ggInfluence(res1.tc3.lr01)"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prediction.html","id":"perform-simple-predictions-from-a-pre-fit-sdm-model","dir":"Articles","previous_headings":"","what":"Perform simple predictions from a pre-fit SDM model","title":"4. Simple prediction","text":"Load previously fit SDM model","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prediction.html","id":"predict-to-previously-observed-locations","dir":"Articles","previous_headings":"Perform simple predictions from a pre-fit SDM model","what":"Predict to previously observed locations","title":"4. Simple prediction","text":"different forecast horizon?","code":"# Predict at 1-year forecast horizon horizon <- 1  inSamplePred <- scoreSDM()  # plot the predictions and observations # Predict at 3-year forecast horizon horizon <- 3  inSamplePred <- scoreSDM()  # plot the predictions and observations"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prediction.html","id":"predict-to-the-full-spatial-surface","dir":"Articles","previous_headings":"Perform simple predictions from a pre-fit SDM model","what":"Predict to the full spatial surface","title":"4. Simple prediction","text":"","code":"# construct a mesh of evenly-spaced points within the sample frame #!!RW: I think this is what Nerea did in 3_Predict_native.R"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prediction.html","id":"compare-the-skill-for-each-prediction","dir":"Articles","previous_headings":"Perform simple predictions from a pre-fit SDM model","what":"Compare the skill for each prediction","title":"4. Simple prediction","text":"","code":"# also compare to a persistence assumption  # Call testSkillSDM to assess the skill of the X-year forecast for this SDM sdmSkill <- testSkillSDM(mod = mod, targetName = targetName, aucCutoff = aucCutoff, usePersistence = FALSE) # Add noYrs and sdmType to output sdmSkill$noYrsTrain <- noYrs sdmSkill$terminalYr <- max(subObs$year) - yrsToForecast  sdmSkill$sdmType <- sdmType    # spatial plot of difference in prediction skill and observations"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"prepare-an-example-dataset-for-specied-distribution-modeling","dir":"Articles","previous_headings":"","what":"Prepare an example dataset for specied distribution modeling","title":"1. Example data preparation workflow","text":"prepare sample dataset create species distribution model northern anchovy using publicly available NOAA SWFSC California Current Ecosystem Survey (CCES) acoustic-trawl sampling observations.","code":""},{"path":[]},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"libraries","dir":"Articles","previous_headings":"Prepare an example dataset for specied distribution modeling > Dependencies","what":"Libraries","title":"1. Example data preparation workflow","text":"","code":"library(rerddap) library(tidyr) library(lubridate) library(here) library(sf) library(dplyr) library(corrplot) library(ggplot2)"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"helper-functions-to-extract-environmental-predictors","dir":"Articles","previous_headings":"Prepare an example dataset for specied distribution modeling > Dependencies","what":"Helper functions to extract environmental predictors","title":"1. Example data preparation workflow","text":"rely environmental netcdfs, stored different places! Note hardcoded Jessie’s repo - change integrated package.","code":"#source(\"/Users/admin/Documents/GitHub/CCSEcolForecasts/R/getROMS.R\") # ROMS ocean model variables # source(\"./R/getMOM6_gridded.R\") # MOM6 ocean model variables #source(\"/Users/admin/Documents/GitHub/CCSEcolForecasts/R/getCMEMS_l4chl.R\") # CMEMS daily L4 chl #source(\"/Users/admin/Documents/GitHub/CCSEcolForecasts/R/getDistLand.R\") # Distance to nearst land based on coast shp #source(\"/Users/admin/Documents/GitHub/CCSEcolForecasts/R/getBathym.R\") # ETOPO bathymetry"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"step-1-clean-cces-data","dir":"Articles","previous_headings":"Prepare an example dataset for specied distribution modeling","what":"Step 1: Clean CCES data","title":"1. Example data preparation workflow","text":"First, start downloading cleaning CCES data. Download latest SWFSC CPS trawl observations ERDDAP Show dataset columns (also see https://coastwatch.pfeg.noaa.gov/erddap/tabledap/FRDCPSTrawlLHHaulCatch.html) Download data.","code":"dataInfo <- info('FRDCPSTrawlLHHaulCatch', url = 'https://coastwatch.pfeg.noaa.gov/erddap/') dataInfo # Metadata ## <ERDDAP(TM) info> FRDCPSTrawlLHHaulCatch  ##  Base URL: https://coastwatch.pfeg.noaa.gov/erddap  ##  Dataset Type: tabledap  ##  Variables:   ##      collection:  ##          Range: 2003, 4897  ##      cruise:  ##          Range: 200307, 202506  ##      haul:  ##          Range: 1, 247  ##      haulback_time:  ##          Range: 1.05772524E9, 1.757495799E9  ##          Units: seconds since 1970-01-01T00:00:00Z  ##      itis_tsn:  ##          Range: 48738, 16182800  ##      latitude:  ##          Range: 28.6513, 54.3997  ##          Units: degrees_north  ##      longitude:  ##          Range: -134.0793, -114.7928  ##          Units: degrees_east  ##      presence_only:  ##      remaining_weight:  ##          Range: 0.0, 31818.0  ##          Units: kg  ##      scientific_name:  ##      ship:  ##      ship_spd_through_water:  ##          Range: 0.0, 4.9  ##          Units: knot  ##      stop_latitude:  ##          Range: 28.6548, 54.4157  ##      stop_longitude:  ##          Range: -134.0325, -114.8483  ##      subsample_count:  ##          Range: 0, 20610  ##      subsample_weight:  ##          Range: 0.0, 2800.0  ##          Units: kg  ##      surface_temp:  ##          Range: 0.0, 185.0  ##          Units: degree C  ##      surface_temp_method:  ##      time:  ##          Range: 1.05772338E9, 1.757493994E9  ##          Units: seconds since 1970-01-01T00:00:00Z cols <- info('FRDCPSTrawlLHHaulCatch')$variables cols ##             variable_name data_type                actual_range ## 1              collection       int                  2003, 4897 ## 2                  cruise       int              200307, 202506 ## 3                    haul       int                      1, 247 ## 4           haulback_time    double 1.05772524E9, 1.757495799E9 ## 5                itis_tsn       int             48738, 16182800 ## 6                latitude     float            28.6513, 54.3997 ## 7               longitude     float        -134.0793, -114.7928 ## 8           presence_only    String                             ## 9        remaining_weight     float                0.0, 31818.0 ## 10        scientific_name    String                             ## 11                   ship    String                             ## 12 ship_spd_through_water     float                    0.0, 4.9 ## 13          stop_latitude     float            28.6548, 54.4157 ## 14         stop_longitude     float        -134.0325, -114.8483 ## 15        subsample_count       int                    0, 20610 ## 16       subsample_weight     float                 0.0, 2800.0 ## 17           surface_temp     float                  0.0, 185.0 ## 18    surface_temp_method    String                             ## 19                   time    double 1.05772338E9, 1.757493994E9 cps <- tabledap(dataInfo, fields = cols$variable_name) ## info() output passed to x; setting base url to: https://coastwatch.pfeg.noaa.gov/erddap ## Warning in set_units(temp_table, dds): NAs introduced by coercion head(cps) ## <ERDDAP tabledap> FRDCPSTrawlLHHaulCatch ##    Path: [/tmp/RtmpTfHYE2/R/rerddap/ec2bef583db1dd80f915a36408960a9b.csv] ##    Last updated: [2026-01-16 21:57:50.676622] ##    File size:    [3.65 mb] ## # A tibble: 6 × 19 ##   collection cruise  haul haulback_time itis_tsn latitude longitude ##        <int>  <int> <int>         <dbl>    <int>    <dbl>     <dbl> ## 1       2003 200307     1            NA    82367     43.0     -125. ## 2       2003 200307     1            NA    82371     43.0     -125. ## 3       2003 200307     1            NA   159643     43.0     -125. ## 4       2003 200307     1            NA   161729     43.0     -125. ## 5       2003 200307     1            NA   161828     43.0     -125. ## 6       2003 200307     1            NA   168586     43.0     -125. ## # ℹ 12 more variables: presence_only <chr>, remaining_weight <dbl>, ## #   scientific_name <chr>, ship <chr>, ship_spd_through_water <dbl>, ## #   stop_latitude <dbl>, stop_longitude <dbl>, subsample_count <int>, ## #   subsample_weight <dbl>, surface_temp <dbl>, surface_temp_method <chr>, ## #   time <dttm>"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"step-2-process-fields","dir":"Articles","previous_headings":"Prepare an example dataset for specied distribution modeling","what":"Step 2: Process fields","title":"1. Example data preparation workflow","text":"NA values sub-samples mean zeroes. Get mean longitude latitude  Add date column Jessie: getting warnings running cpsMatrix code. Check Rob. Add useful columns Subset just anchovy","code":"cps$subsample_count <- ifelse(is.na(cps$subsample_count), 0, cps$subsample_count) cps$subsample_weight <- ifelse(is.na(cps$subsample_weight), 0, cps$subsample_weight) # Use numbers and weights to show presence/absence  cps$pres <- ifelse(cps$subsample_count > 0 | cps$subsample_weight > 0, 1, 0) head(cps) ## <ERDDAP tabledap> FRDCPSTrawlLHHaulCatch ##    Path: [/tmp/RtmpTfHYE2/R/rerddap/ec2bef583db1dd80f915a36408960a9b.csv] ##    Last updated: [2026-01-16 21:57:50.676622] ##    File size:    [3.65 mb] ## # A tibble: 6 × 20 ##   collection cruise  haul haulback_time itis_tsn latitude longitude ##        <int>  <int> <int>         <dbl>    <int>    <dbl>     <dbl> ## 1       2003 200307     1            NA    82367     43.0     -125. ## 2       2003 200307     1            NA    82371     43.0     -125. ## 3       2003 200307     1            NA   159643     43.0     -125. ## 4       2003 200307     1            NA   161729     43.0     -125. ## 5       2003 200307     1            NA   161828     43.0     -125. ## 6       2003 200307     1            NA   168586     43.0     -125. ## # ℹ 13 more variables: presence_only <chr>, remaining_weight <dbl>, ## #   scientific_name <chr>, ship <chr>, ship_spd_through_water <dbl>, ## #   stop_latitude <dbl>, stop_longitude <dbl>, subsample_count <dbl>, ## #   subsample_weight <dbl>, surface_temp <dbl>, surface_temp_method <chr>, ## #   time <dttm>, pres <dbl> cps$lon <- rowMeans(   cbind(     as.numeric(cps$longitude),     as.numeric(cps$stop_longitude)   ),   na.rm = TRUE ) cps$lat <- rowMeans(   cbind(     as.numeric(cps$latitude),     as.numeric(cps$stop_latitude)   ),   na.rm = TRUE ) #cps$lon <- rowMeans(cps[c(\"longitude\", \"stop_longitude\")])  #cps$lat <- rowMeans(cps[c(\"latitude\", \"stop_latitude\")]) plot(cps$lon, cps$lat) # Quick for any outlier locations #names(cps) cps$date <- as.Date(cps$time) # Remove a few daytime samples cps$hr <- hour(cps$time - hours(7)) # times originally in UTC cps$dn <- ifelse(cps$hr < 6.1 | cps$hr > 17.9, \"night\", \"day\")  cps1 <- subset(cps, dn == \"night\")   cps1 <- cps1 %>% as.data.frame() names(cps1) ##  [1] \"collection\"             \"cruise\"                 \"haul\"                   ##  [4] \"haulback_time\"          \"itis_tsn\"               \"latitude\"               ##  [7] \"longitude\"              \"presence_only\"          \"remaining_weight\"       ## [10] \"scientific_name\"        \"ship\"                   \"ship_spd_through_water\" ## [13] \"stop_latitude\"          \"stop_longitude\"         \"subsample_count\"        ## [16] \"subsample_weight\"       \"surface_temp\"           \"surface_temp_method\"    ## [19] \"time\"                   \"pres\"                   \"lon\"                    ## [22] \"lat\"                    \"date\"                   \"hr\"                     ## [25] \"dn\" head(cps1) ##   collection cruise haul haulback_time itis_tsn latitude longitude ## 2       2003 200307    1            NA    82367  42.9816 -124.8413 ## 3       2003 200307    1            NA    82371  42.9816 -124.8413 ## 4       2003 200307    1            NA   159643  42.9816 -124.8413 ## 5       2003 200307    1            NA   161729  42.9816 -124.8413 ## 6       2003 200307    1            NA   161828  42.9816 -124.8413 ## 7       2003 200307    1            NA   168586  42.9816 -124.8413 ##   presence_only remaining_weight        scientific_name ship ## 2             N              NaN               Teuthida   FR ## 3             N              NaN Doryteuthis opalescens   FR ## 4             N              NaN                Salpida   FR ## 5             N              NaN        Sardinops sagax   FR ## 6             N              NaN       Engraulis mordax   FR ## 7             N              NaN  Trachurus symmetricus   FR ##   ship_spd_through_water stop_latitude stop_longitude subsample_count ## 2                    3.5       43.0006       -124.893               1 ## 3                    3.5       43.0006       -124.893               3 ## 4                    3.5       43.0006       -124.893               0 ## 5                    3.5       43.0006       -124.893               0 ## 6                    3.5       43.0006       -124.893               0 ## 7                    3.5       43.0006       -124.893               0 ##   subsample_weight surface_temp surface_temp_method                time pres ## 2             0.01         13.3              bucket 2003-07-09 04:03:00    1 ## 3             0.03         13.3              bucket 2003-07-09 04:03:00    1 ## 4             0.10         13.3              bucket 2003-07-09 04:03:00    1 ## 5             0.01         13.3              bucket 2003-07-09 04:03:00    1 ## 6             0.05         13.3              bucket 2003-07-09 04:03:00    1 ## 7            26.00         13.3              bucket 2003-07-09 04:03:00    1 ##         lon     lat       date hr    dn ## 2 -124.8672 42.9911 2003-07-09 21 night ## 3 -124.8672 42.9911 2003-07-09 21 night ## 4 -124.8672 42.9911 2003-07-09 21 night ## 5 -124.8672 42.9911 2003-07-09 21 night ## 6 -124.8672 42.9911 2003-07-09 21 night ## 7 -124.8672 42.9911 2003-07-09 21 night # Convert to wide format cpsMatrix <- pivot_wider(cps1,                           names_from = scientific_name,                           values_from = pres, id_cols = c(cruise, haul, lon, lat, date, dn),                           values_fill = list(values = 0)) ## Warning: Values from `pres` are not uniquely identified; output will contain list-cols. ## • Use `values_fn = list` to suppress this warning. ## • Use `values_fn = {summary_fun}` to summarise duplicates. ## • Use the following dplyr code to identify duplicates. ##   {data} |> ##   dplyr::summarise(n = dplyr::n(), .by = c(cruise, haul, lon, lat, date, dn, ##   scientific_name)) |> ##   dplyr::filter(n > 1L) # Convert NA to 0  cpsMatrix[is.na(cpsMatrix)] <- 0  head(cpsMatrix) ## # A tibble: 6 × 276 ##   cruise  haul   lon   lat date       dn    Teuthida  `Doryteuthis opalescens` ##    <int> <int> <dbl> <dbl> <date>     <chr> <list>    <list>                   ## 1 200307     1 -125.  43.0 2003-07-09 night <dbl [1]> <dbl [1]>                ## 2 200307     2 -125.  43.0 2003-07-09 night <dbl [1]> <NULL>                   ## 3 200307     3 -125.  43.0 2003-07-09 night <dbl [1]> <NULL>                   ## 4 200307     4 -125.  43.0 2003-07-09 night <NULL>    <NULL>                   ## 5 200307     5 -125.  43.0 2003-07-09 night <NULL>    <NULL>                   ## 6 200307     6 -125.  43.0 2003-07-09 night <NULL>    <NULL>                   ## # ℹ 268 more variables: Salpida <list>, `Sardinops sagax` <list>, ## #   `Engraulis mordax` <list>, `Trachurus symmetricus` <list>, ## #   Myctophidae <list>, `Merluccius productus` <list>, ## #   `Cololabis saira` <list>, `Oncorhynchus kisutch` <list>, ## #   `Scomber japonicus` <list>, `Icichthys lockingtoni` <list>, ## #   `Prionace glauca` <list>, Oncorhynchus <list>, `Thunnus alalunga` <list>, ## #   Animalia <list>, `Clupea pallasii` <list>, `Alosa sapidissima` <list>, … cpsMatrix$survey <- \"cps\" cpsAnch <- cpsMatrix[c(\"survey\", \"cruise\", \"haul\", \"lon\", \"lat\", \"date\", \"Engraulis mordax\")]  colnames(cpsAnch)[ncol(cpsAnch)] <- \"anchPA\"  # Add a unique ID cpsAnch$id <- seq(1:nrow(cpsAnch)) # used as 'obs' bellow head(cpsAnch) ## # A tibble: 6 × 8 ##   survey cruise  haul   lon   lat date       anchPA       id ##   <chr>   <int> <int> <dbl> <dbl> <date>     <list>    <int> ## 1 cps    200307     1 -125.  43.0 2003-07-09 <dbl [1]>     1 ## 2 cps    200307     2 -125.  43.0 2003-07-09 <NULL>        2 ## 3 cps    200307     3 -125.  43.0 2003-07-09 <NULL>        3 ## 4 cps    200307     4 -125.  43.0 2003-07-09 <NULL>        4 ## 5 cps    200307     5 -125.  43.0 2003-07-09 <NULL>        5 ## 6 cps    200307     6 -125.  43.0 2003-07-09 <NULL>        6"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"step-3-pull-phys--covariate-data","dir":"Articles","previous_headings":"Prepare an example dataset for specied distribution modeling","what":"Step 3: Pull phys. covariate data","title":"1. Example data preparation workflow","text":"Next, pull together physical covariate data public online databases. Note, Jessie run don’t Hinchliffe dataset repo. now, load output local file","code":"# Load datasets: these currently live in project ./data # coast <- sf::read_sf(\"./data/EPOCoast60_noGI.shp\") # The coast shp coast <- tigris::coastline() #!!RW: have to check if this will work for demonstration purposes anch <- read.csv(\"/Users/admin/Documents/GitHub/CCSEcolForecasts/data/Hinchliffe_CSNA_timeseries_19652023.csv\") # Anchovy SSB from Hincliffe et al. 2025  # To get a \"persistence\" forecast, we extract environmental variables from the year before the sampling date # lag = 0 in the function extracts variables at the sampling date, lag = 1 extracts from the year before # This is clunky but here I'm running the function twice to get lag = 0 and lag = 1, then joining the outputs # Careful with lag > 0, could potentially give dates outside the temporal range of environmental datasets #!!RW: generalize extractEnvVars() to specifiy which covariates to pull in #      - OR just call the specific get*() fxns to make things more explicit envExtractLag0 <- extractEnvVars(obs = obs, lag = 0) envExtractLag1 <- extractEnvVars(obs = obs, lag = 1) # Adjust colnames: clunky #!!RW: Clean this up colnames(envExtractLag1) <- paste0(colnames(envExtractLag1), \"_lag1\") # Adjust with lag number # Figure out col index where environmental covariates start startCol <- ncol(obs) + 1 envExtract <- cbind(envExtractLag0, envExtractLag1[, startCol: ncol(envExtractLag1)]) # Save the output saveRDS(envExtract, \"./data/combinedBioObs_envExtracted.rds\") f <- system.file(\"extdata\", \"combinedBioObs_envExtracted.rds\",                  package = \"sdmclimateforecasts\") envExtract <- readRDS(f) head(envExtract) ##   survey cruise haul       lon      lat       date anchPA id      ild      sst ## 1    cps 200307    1 -124.8672 42.99110 2003-07-09      1  1 2.404042 12.67080 ## 2    cps 200307    2 -124.9490 43.00345 2003-07-09      0  2 2.404042 12.67080 ## 3    cps 200307    3 -125.0348 43.01560 2003-07-09      0  3 3.228197 12.79822 ## 4    cps 200307    4 -125.1154 43.00045 2003-07-09      0  4 3.503623 13.27315 ## 5    cps 200307    5 -125.2075 43.00005 2003-07-09      0  5 3.716197 13.91262 ## 6    cps 200307    6 -125.3057 43.00300 2003-07-09      0  6 3.734344 14.40529 ##      sst_sd     ssh_sd         eke    logEKE       chl      logChl distLand ## 1 1.0602433 0.01866543 0.027121617 -3.607424 0.9818046 -0.01836293 29629.38 ## 2 1.0602433 0.01866543 0.039371376 -3.234716 0.7811583 -0.24697752 36043.10 ## 3 0.9719992 0.02196990 0.009381834 -4.668980 0.7359316 -0.30661811 42827.86 ## 4 0.8266193 0.02738416 0.006201829 -5.082911 0.9444131 -0.05719161 48125.07 ## 5 0.7526832 0.03357841 0.011109728 -4.499934 1.1147090  0.10859338 55161.98 ## 6 0.6279386 0.04007801 0.030783334 -3.480782 1.0380287  0.03732344 62894.87 ##       bathym year anchssb ild_lag1 sst_lag1 sst_sd_lag1 ssh_sd_lag1   eke_lag1 ## 1  -254.0408 2003  466616 6.104747 14.48057   1.2994136  0.01999982 0.01568088 ## 2  -761.1837 2003  466616 6.104747 14.48057   1.2994136  0.01999982 0.01264101 ## 3 -1279.6939 2003  466616 6.630548 15.16734   1.0764430  0.02447072 0.03796448 ## 4 -1809.4694 2003  466616 7.330258 15.66488   0.8026699  0.02746283 0.06900047 ## 5 -2324.0000 2003  466616 7.648054 15.94528   0.6320010  0.02865770 0.10133756 ## 6 -3067.7959 2003  466616 7.912380 16.18897   0.5238957  0.02855562 0.11550667 ##   logEKE_lag1   chl_lag1 logChl_lag1 distLand_lag1 bathym_lag1 year_lag1 ## 1   -4.155313 12.6281660  2.53592971      29629.38   -254.0408      2002 ## 2   -4.370809  7.4417920  2.00711168      36043.10   -761.1837      2002 ## 3   -3.271104  4.0881181  1.40808475      42827.86  -1279.6939      2002 ## 4   -2.673642  1.3723352  0.31651382      48125.07  -1809.4694      2002 ## 5   -2.289298  0.9124697 -0.09160038      55161.98  -2324.0000      2002 ## 6   -2.158427  1.0364990  0.03584867      62894.87  -3067.7959      2002 ##   anchssb_lag1 ## 1       339465 ## 2       339465 ## 3       339465 ## 4       339465 ## 5       339465 ## 6       339465"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/prepare-data.html","id":"step-4-relate-env-var--to-anchovy-obs","dir":"Articles","previous_headings":"Prepare an example dataset for specied distribution modeling","what":"Step 4: Relate env var. to anchovy obs","title":"1. Example data preparation workflow","text":"can investigate variables relate anchovy observations CCES dataset.  Plot correlations  Biplots covariate space  create set training test subsets observational data later use. TBD.","code":"# plot example time series envExtract %>% ggplot(aes(x = year, y = sst)) +   geom_point() +   facet_wrap(~anchPA) ## Warning: Removed 222 rows containing missing values or values outside the scale range ## (`geom_point()`). corrMat <- cor(envExtract %>% select(lon, lat, anchPA, ild, sst, eke, chl, bathym, anchssb), use = \"pairwise.complete.obs\") pTest <- cor.mtest(envExtract %>%                       select(lon, lat, anchPA, ild, sst, eke, chl, bathym, anchssb),                     alternative = \"two.sided\",                     method = \"pearson\") corrplot(corrMat, p.mat = pTest$p, sig.level = 0.05, insig = \"blank\",          order = 'hclust', hclust.method = \"ward.D2\", #\"centroid\", #\"single\", #          tl.col = 'black', type = \"lower\",          cl.ratio = 0.1, tl.srt = 45, tl.cex = 0.6, #mar = c(0.1, 0.1, 0.1, 0.1),           addrect = 6, rect.col = \"green\", diag = FALSE) envExtract %>% filter(anchPA == 1) %>%   ggplot(aes(x = ild, y = sst)) +   geom_point() ## Warning: Removed 24 rows containing missing values or values outside the scale range ## (`geom_point()`)."},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/articles/skill.html","id":"perform-leave-future-out-cross-validation-to-test-forecast-skill-of-an-sdm","dir":"Articles","previous_headings":"","what":"Perform Leave-Future-Out cross-validation to test forecast skill of an SDM","title":"5. Test forecast skill","text":"Load full dataset Test 5-year forecast skill #!!RW: Need define/load peelSDM() fxn EvaluateFit.R Summarize skill 6 retrospective peels Comparative plots forecast performance","code":"# Load biological observation data with environmental covariates extracted # This is now updated to include variables extracted at a lag of 1 year, which are used to calculate # persistence skill (variables labeled with \"_lag1\") # Is assumed that the columns \"lon\", \"lat\", and \"date\" are present and complete obs <- readRDS(\"./data/combinedBioObs_envExtracted.rds\") # If observations don't have a \"year\", \"month\" and \"quarter\" columns, add them. Will need a \"date\" column to get them if(!\"year\" %in% colnames(obs)) {   obs$year <- year(obs$date)   } if(!\"month\" %in% colnames(obs)) {   obs$month <- month(obs$date)   } if(!\"quarter\" %in% colnames(obs)) {   obs$quarter <- quarter(obs$date)   }  # I'm missing predictor variables for 2024, so I'm trimming that year off obs <- subset(obs, year <= 2023) # Define the target variable: here presence/absence of anchovy obs$pa <- obs$anchPA ########################################################################################################### # Define some parameters for building the SDM/s # Note that for now the user supplies the SDM-specific parameters. Optimizing parameters for an SDM (esp. a BRT)  # when you don't have good information on dataset size, number of variables, prevalence rate etc. is difficult # This could be improved in future iterations of this code! sdmType <- \"brt\" # Can currently be \"gam\" or \"brt\" k <- 4 # Number of knots for a GAM. Will be ignored if not building a GAM tc <- 3 # tree complexity for a BRT. Will be ignored if not building a BRT lr <- 0.02 # learning rate for a BRT. Will be ignored if not building a BRT max.trees <- 10000 # max trees for a BRT. Will be ignored if not building a BRT varNames <- c(\"sst\", \"ild\", \"sst_sd\", \"ssh_sd\", \"logChl\", \"logEKE\", \"distLand\", \"anchssb\") # Names of predictors in the SDM targetName <- \"pa\" # Target variable for the SDM aucCutoff <- 10 # If less observations than this cutoff within a month/season etc., AUC will not be calculated  ########################################################################################################### # An example: 5-year forecast skill for anchovy SDM where training data include between 10 and 15 years of data  yrsToTrain <- seq(10, 15)  yrsToForecast <- 5 # Optional: do we want to assess persistence skill? That is: can last year's environmental conditions # predict this years species distributions? includePersistence <- TRUE output <- vector(mode = \"list\", length = length(yrsToTrain)) # We will save results to a list (a list of lists) for (j in 1:length(output)) {   output[[j]] <- peelSDM(obs = obs, sdmType = sdmType, varNames = varNames, targetName = targetName,                          k = k, tc = tc, lr = lr, max.trees = max.trees, noYrs = yrsToTrain[j],                           yrsToForecast = yrsToForecast, includePersistence = includePersistence) } ########################################################################################################### # Some quick plots of SDM skill with varying lengths of training data, and varying levels of spatial/temporal aggregation # for each forecasting horizon separately # First reshape the output list so is easier to work with suppressWarnings(rm(sdmSkill1, sdmSkillAll)) for(b in 1:length(output)) {    sdmSkill1 <- output[[b]]$sdmSkill   if(exists(\"sdmSkillAll\")) {     sdmSkillAll <- rbind(sdmSkillAll, sdmSkill1)   } else {     sdmSkillAll <- sdmSkill1   } }  # Aggregate so skills are averaged across different months/seasons. \"year\" is the test year sdmSkillAllAgg <- aggregate(auc ~ time + space + noYrsTrain + terminalYr + year, sdmSkillAll, FUN = mean, na.rm = TRUE) # Add a field converting year to forecast time horizon sdmSkillAllAgg$forecastHorizon <- sdmSkillAllAgg$year - sdmSkillAllAgg$terminalYr # Plot showing mean AUC skill across all terminal years by spatial and temporal aggregation and forecast horizon ggplot(sdmSkillAllAgg) +    geom_boxplot(aes(x = forecastHorizon, y = auc, group = factor(forecastHorizon), fill = forecastHorizon)) +   scale_fill_viridis(\"Forecast \\nHorizon\", option = \"mako\") + xlab(\"Forecast Horizon (Years)\") + ylab(\"Forecast AUC\") +    scale_y_continuous(limits = c(0, 1), oob = rescale_none) + theme_bw() + facet_grid(time ~ space, scales = \"free\")  ########################################################################################################### # An additional plot showing real/contemporary skill vs persistence skill, if you also calculated and returned those results # Reshape the persistence skill outputs suppressWarnings(rm(sdmSkillPers1, sdmSkillPersAll)) for(b in 1:length(output)) {    sdmSkillPers1 <- output[[b]]$sdmSkillPers   if(exists(\"sdmSkillPersAll\")) {     sdmSkillPersAll <- rbind(sdmSkillPersAll, sdmSkillPers1)   } else {     sdmSkillPersAll <- sdmSkillPers1   } }  # Aggregate so skills are averaged across different months/seasons. \"year\" is the test year sdmSkillPersAllAgg <- aggregate(auc ~ time + space + noYrsTrain + terminalYr + year, sdmSkillPersAll, FUN = mean, na.rm = TRUE) # Add a field converting year to forecast time horizon sdmSkillPersAllAgg$forecastHorizon <- sdmSkillPersAllAgg$year - sdmSkillPersAllAgg$terminalYr # Combine with skill assessment from contemporary data sdmSkillAllAgg$forecastType <- \"contemporary\" sdmSkillPersAllAgg$forecastType <- \"persistence\" sdmSkillBoth <- rbind(sdmSkillAllAgg, sdmSkillPersAllAgg) sdmSkillBoth$horizonType <- interaction(sdmSkillBoth$forecastHorizon, sdmSkillBoth$forecastType)  # A plot showing skill by aggregation level and forecast horizon, now comparing contemporary prediction vs. persistence ggplot() +    geom_boxplot(data = sdmSkillBoth, aes(x = forecastHorizon, y = auc, group = factor(horizonType), fill = forecastHorizon)) +   scale_fill_viridis(\"Forecast \\nHorizon\", option = \"mako\") + xlab(\"Forecast Horizon (Years)\") + ylab(\"Forecast AUC\") +    scale_y_continuous(limits = c(0, 1), oob = rescale_none) + theme_bw() + facet_grid(time ~ space)"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jessica . Bolin. Author, maintainer. Rob Wildemuth. Author.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bolin J, Wildemuth R (2026). sdmclimateforecasts: California Current System Ecological Forecasts. R package version 0.0.1, https://github.com/JessicaBolin/sdmclimateforecasts.","code":"@Manual{,   title = {sdmclimateforecasts: California Current System Ecological Forecasts},   author = {Jessica A. Bolin and Rob Wildemuth},   year = {2026},   note = {R package version 0.0.1},   url = {https://github.com/JessicaBolin/sdmclimateforecasts}, }"},{"path":[]},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"California Current System Ecological Forecasts","text":"sdmclimateforecasts began part NOAA Fisheries’ Changing Ecosystems Fisheries Initiative (CEFI) collaboration code modeling among researchers NOAA, University California Santa Cruz Davis, University Washington. OBJECTIVE produce framework evaluate communicate skill forecast models integrate ecological processes use operational ecosystem-based fisheries management. code repository supports production species distribution models (SDMs) intent creating forecast products three spatial case study applications: index anchovy availability support evaluations California sea lion breeding success Southern California Bight (see Fennie et al. 2023 details) habitat model swordfish distribution based West Coast drift gillnet fishery catches index risk Pacific sardine bycatch Pacific mackerel fishery AIMS : Assess forecast skill SDMs using NOAA’s Physical Sciences Laboratory MOM6 oceanographic products Create generalized skill assessment framework use within West Coast CEFI Decision Support Team beyond Communicate confidence forecast model products various spatiotemporal scales","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"California Current System Ecological Forecasts","text":"can install development version GitHub via:","code":"# install.packages(\"devtools) devtools::install_github(\"JessicaBolin/sdmclimateforecasts\")"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"California Current System Ecological Forecasts","text":"Check Getting Started article see examples deploy functions.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"California Current System Ecological Forecasts","text":"full bibliographic reference: insert citation","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting help","title":"California Current System Ecological Forecasts","text":"two main places get help sdmclimateforecasts: Email package maintainer directly jabbolin()ucdavis(dot)edu. Submit Issue via GitHub repository.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getBathym.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract bathymetry — getBathym","title":"Extract bathymetry — getBathym","text":"function extracts bathymetry lat/lons interest directly ERDAPP.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getBathym.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract bathymetry — getBathym","text":"","code":"getBathym(points, desired.diameter = 0.7, func = \"mean\")"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getBathym.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract bathymetry — getBathym","text":"points Data frame. Input data frame columns labeled lon lat desired.diameter Numeric. Defaults 0.7˚. Results 7x7 (= 49) pixel box (0.7x0.7˚) around central location. func Character. Default \"mean\", can change \"sd\".","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getBathym.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract bathymetry — getBathym","text":"five column data frame bathymetry extracted lat/lons interest.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getBathym.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract bathymetry — getBathym","text":"Can slow large datasets.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getBathym.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract bathymetry — getBathym","text":"","code":"points <- expand.grid(   lon = seq(-132, -122, by = 2),   lat = seq(32, 40, by = 2) )  testbathym <- getBathym(   points = points,   desired.diameter = 0.7,   func = \"mean\" ) #> Registered S3 method overwritten by 'httr': #>   method           from   #>   print.cache_info hoardr #>    |                                                                               |                                                                      |   0%   |                                                                               |==                                                                    |   3%  head(testbathym) #>    lon lat bathym_mean_0.7 bathym_count_0.7 #> 1 -132  32       -4550.447             1849 #> 2 -130  32       -4502.723             1849 #> 3 -128  32       -4279.393             1849 #> 4 -126  32       -4207.784             1849 #> 5 -124  32       -4198.538             1849 #> 6 -122  32       -4043.458             1849"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getCMEMS_l4chl.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","title":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","text":"Requires global 4km netcdfs downloaded CMEMS L4 global ocean color product stored locally.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getCMEMS_l4chl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","text":"","code":"getCMEMS_l4chl(points, desired.diameter = 0.7, func = c(\"mean\", \"sd\"), nc.path)"},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getCMEMS_l4chl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","text":"points Data frame.  Requires input data frame columns labeled \"lon\" (degrees west), \"lat\", \"date\" (formatted dates). desired.diameter Numeric. Default = 0.7. Results 0.7 x 0.7 degree box around central location func Character. \"mean\" \"sd\" nc.path Character. File path daily netCDF's stored","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getCMEMS_l4chl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","text":"five column data frame chla extracted lat/lons/dates interest.","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getCMEMS_l4chl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","text":"Unlike ROMS, CMEMS chl resolution defined km, degrees. pixel sizes vary slightly latitude. keep workflow consistent existing FRD/ESD workflows, assume pixel sizes 0.0416667 X 0.0416667. may accurate high latitudes!","code":""},{"path":"https://jessicabolin.github.io/sdmclimateforecasts/reference/getCMEMS_l4chl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract CMEMS level 4 daily chl interpolated product — getCMEMS_l4chl","text":"","code":"points <- data.frame(\"lon\" = c(-130, -125, -124, -130, -125, -124, -130, -125, -124, -180), \"lat\" = c(45, 34, 32, 40, 45, 34, 32, 40, 36, 41), \"date\" = as.Date(\"2011-04-04\")) desired.diameter <- 0.7 # Note! This means a 0.7x0.7 degree box func <- \"mean\" # mean or sd nc.path <- \"/Users/admin/Downloads\" #getCMEMS_l4chl(points, desired.diameter = 0.7, \"mean\", nc.path)"}]
